{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian analysis for pulsar dispersion measure (DM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for the UW 2018 Computational Physics course, module II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction, data and paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test a model for the dispersion measure of pulsars (DM). The model includes an astrophysical contribution\n",
    "\\begin{equation}\n",
    "DM=DM_{\\text{astro}}+DM_{\\text{dm}} \\, .\n",
    "\\end{equation}\n",
    "\n",
    "For the astrophysical contribution, we know\n",
    "\\begin{equation}\n",
    "DM_{\\text{astro}}= n_e d \\, ,\n",
    "\\end{equation}\n",
    "where $n_e$ is the electron density, estimated through the [YMW16](http://119.78.162.254/dmodel/) electron-density model, and $d$ is the distance determined with parallax.\n",
    "\n",
    "For the dark matter contribution, we assume milli-charged dark matter of mass $m_{\\text{dm}}$ and charge $q=e \\epsilon$,\n",
    "\\begin{equation}\n",
    "DM_{\\text{astro}}= m_e \\frac{\\epsilon^2}{m_{\\text{dm}}^2} \\rho d \\, ,\n",
    "\\end{equation}\n",
    "where $\\rho$ is the dark matter energy density of the Milky Way, assumed to be uniform.\n",
    "\n",
    "We first upload the data set provided by the ATNF pulsar catalogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_data()\n",
    "#n     PX       PXerr       DM        DMerr         ne\n",
    "# when the error was missing (0) I set it to the last digit    \n",
    "datatable= \"\"\"\n",
    "1     0.130     2.0e-02     34.98     1.6e-03     0.000062\n",
    "2     0.770     2.3e-01     6.48     6.0e-04     0.009036\n",
    "3     0.213     0.001     24.37     3.0e-03     0.000836    \n",
    "4     0.710     1.7e-01     9.02     7.0e-05     0.007827\n",
    "5     0.300     1.0e-01     22.18     5.0e-03     0.004931\n",
    "6     0.700     2.0e-01     10.76     1.5e-03     0.008292\n",
    "7     2.530     7.0e-02     3.14     0.01     0.020564\n",
    "8     0.600     2.0e-01     13.31     2.5e-03     0.044781\n",
    "9     0.810     3.0e-02     10.39     0.01     0.016935\n",
    "10     2.770     7.0e-02     3.18     1.6e-04     0.015919\n",
    "11     0.370     1.2e-01     24.63     1.8e-04     0.012689\n",
    "12     0.470     3.0e-02     19.62     3.0e-04     0.004691\n",
    "13     0.400     2.0e-01     23.92     9.0e-05     0.011004\n",
    "14     0.160     9.0e-02     61.25     5.0e-03     0.003488\n",
    "15     0.860     1.8e-01     11.62     2.0e-05     0.009608\n",
    "16     4.200     1.4e+00     2.38     1.9e-01     0.009246\n",
    "17     0.900     4.0e-01     11.11     1.1e-04     0.024026\n",
    "18     0.930     8.0e-02     10.92     6.0e-03     0.008167\n",
    "19     0.731     2.2e-02     14.32     1.0e-02     0.008209\n",
    "20     1.160     8.0e-02     9.25     5.3e-04     0.008940\n",
    "\"\"\"\n",
    "    lines = split(datatable, '\\n')\n",
    "    lines = filter(x->(length(x)>0), lines)\n",
    "    lines = filter(x->(length(x)>0), lines)\n",
    "\n",
    "    x = zeros(length(lines))\n",
    "    y = zeros(length(lines))\n",
    "    yerr = zeros(length(lines))\n",
    "    xerr = zeros(length(lines))\n",
    "    ne = zeros(length(lines))\n",
    "    for (i,line) in enumerate(lines)\n",
    "        words = split(line, \"     \")\n",
    "        x[i] = 10^(-3)* parse(Float64, words[2]) # parallax in as\n",
    "        y[i] = parse(Float64, words[4])      # DM in cm^-3 pc\n",
    "        yerr[i] = parse(Float64, words[5])    # DM err\n",
    "        xerr[i] = 10^(-3)* parse(Float64, words[3])   # parallax err\n",
    "        ne[i] = parse(Float64, words[6])   # electron density cm^-3\n",
    "    end\n",
    "    return x,y,xerr,yerr,ne\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,xerr,yerr,ne = get_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the main parameters, and se the units straight:\n",
    "we define the dark matter density, the distance (from the parallax) and its error, the electron mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510998.94609999994"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const rho = 0.3 * 10^9  # DM energy density (eV cm^-3)\n",
    "d =1 ./x      # distance (pc)\n",
    "derr = 1 ./ (x .* x) .* xerr  # distance error (pc);\n",
    "const m_e = 0.5109989461 * 10^6   # electron mass in eV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The eemc ensamble sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to follow is adaptated from the tutorial session 3 of the data analysis course module. \n",
    "\n",
    "We can finally write the likelihood for our hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mymodel_log_likelihood(x, y, sigma, ϵ , m_dm, ne)\n",
    "    return -0.5 * sum(@. (y - (ne + m_e * rho * ϵ * ϵ / m_dm^2 )*x)^2 / sigma^2)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function good_mymodel_lnl(pars)\n",
    "    ϵ,m_dm = pars\n",
    "    mymodel_log_likelihood(d, y, yerr, ϵ , m_dm, ne)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should think about the priors here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function good_mymodel_logprob(pars)\n",
    "    logposterior = 0.\n",
    "    return logposterior + good_mymodel_lnl(pars)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ensemble_sampler(logprob_func, initial, nsteps)\n",
    "    # A translation of the \"emcee\" ensemble sampler by Dan Foreman-Mackey,\n",
    "    # http://arxiv.org/abs/1202.3665.  This is \"Algorithm 3\" in that paper.\n",
    "    #\n",
    "    # *logprob_func* is a log-posterior-probability function of the parameters.\n",
    "    # *initial* must be an array with shape (Nwalkers, Nparams)\n",
    "    # *nsteps* is the desired number of MCMC steps to take\n",
    "    #\n",
    "    nwalkers, nparams = size(initial)\n",
    "    if (nwalkers % 2 != 0)\n",
    "        println(\"NWalkers must be even\")\n",
    "        return\n",
    "    end\n",
    "    chain = zeros((nsteps, nwalkers, nparams))\n",
    "    logprobs = zeros(nwalkers)\n",
    "    nhalf = Int(nwalkers/2)\n",
    "    # Start at initial parameter values\n",
    "    params = initial\n",
    "    for j in 1:nwalkers\n",
    "        logprobs[j] = logprob_func(params[j,:])\n",
    "    end\n",
    "    # \"Stretch move\" parameter\n",
    "    alpha = 2.\n",
    "    # Tried moves\n",
    "    tries = 0\n",
    "    # Accepted moves\n",
    "    accepts = 0\n",
    "    for i in 1:nsteps\n",
    "        # At each step, we first update one half of the ensemble (holding the other half fixed)\n",
    "        # and then updated the second half (holding the first half, with its updated values, fixed)\n",
    "        for half in 0:1\n",
    "            for j in 1:nhalf\n",
    "                # We're going to update the walker at index \"me\":\n",
    "                me = half*nhalf + j\n",
    "                # Draw a walker from the other half of the walkers\n",
    "                other = (1-half)*nhalf + rand(1:nhalf)\n",
    "                # Draw the \"stretch move\" distance\n",
    "                z = ((alpha - 1.) * rand(Float64) + 1)^2 / alpha\n",
    "                # Compute the parameter value -- it is along the line connecting \"me\" and \"other\"\n",
    "                params_new = params[other,:] .+ z .* (params[me,:] - params[other,:])\n",
    "                # Compute the log-prob at the new sample location\n",
    "                logprob_new = logprob_func(params_new)\n",
    "                # There is an extra term that weights the proposal distribution -- required to maintain\n",
    "                # detailed balance.\n",
    "                logproposal = (nparams-1) * log(z)\n",
    "                tries += 1\n",
    "                # Keep this new sample?\n",
    "                if exp(logproposal + logprob_new - logprobs[me]) >= rand(Float64)\n",
    "                    # Keep it!\n",
    "                    params[me,:] = params_new\n",
    "                    logprobs[me] = logprob_new\n",
    "                    accepts += 1\n",
    "                end\n",
    "                # Record the chosen parameter values\n",
    "                chain[i, me, :] .= params[me, :]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return chain, accepts/tries\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial = [10^-7,10^3]  # initial guess for the paramters\n",
    "\n",
    "jumpsizes = ones(length(initial)) * 1e-6\n",
    "nparams = length(initial)\n",
    "\n",
    "nwalkers = 20  # nwalker >> nparams\n",
    "\n",
    "walkers = zeros((nwalkers, nparams)) \n",
    "for i in 1:nwalkers  # We initialize the walkers in a ball around the *initial* location.\n",
    "    walkers[i,:] = initial + randn(nparams) .* jumpsizes\n",
    "end\n",
    "\n",
    "# Call the sampler!\n",
    "nsteps = 200   \n",
    "chain,acceptance = ensemble_sampler(good_mymodel_logprob, walkers, nsteps)\n",
    "acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-38580.32191331429"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(chain)\n",
    "chain[200,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1\n",
    "flat = chain[burnin:end,:,:]\n",
    "nb,nw,np = size(flat)\n",
    "flat = reshape(flat, (nb*nw,np));\n",
    "h1 = histogram(flat[:,1], xlabel=\"epsilon\", label=\"\")\n",
    "h2 = histogram(flat[:,2], xlabel=\"m_dm\", label=\"\")\n",
    "plot(h1,h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram2d(flat[:,1], flat[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
